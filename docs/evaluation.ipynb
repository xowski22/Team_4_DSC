{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric: MAP@10 (Mean Average Precision at 10)\n",
    "\n",
    "This notebook explains the evaluation metric used in our Kaggle competition - **MAP@10** (Mean Average Precision at 10).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¯ What is MAP@10?\n",
    "\n",
    "**MAP@10** is a ranking evaluation metric that measures how well our recommendation system performs by looking at the **top 10 recommendations** for each user.\n",
    "\n",
    "### Key Formula:\n",
    "```\n",
    "MAP@10 = (1/|U|) Ã— Î£(uâˆˆU) AP@10(u)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **U** = Set of all users\n",
    "- **AP@10(u)** = Average Precision at 10 for user u\n",
    "- We take the **average** across all users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ“Š Breaking Down AP@10 (Average Precision at 10)\n",
    "\n",
    "For each individual user **u**, we calculate:\n",
    "\n",
    "```\n",
    "AP@10(u) = (1/min(|R_u|, 10)) Ã— Î£(k=1 to 10) P_u(k) Ã— rel_u(k)\n",
    "```\n",
    "\n",
    "### Components:\n",
    "- **R_u** = Set of relevant (true) items for user u\n",
    "- **P_u(k)** = Precision at position k = (# of relevant items in top k) / k\n",
    "- **rel_u(k)** = 1 if item at position k is relevant, 0 otherwise\n",
    "\n",
    "### ğŸ” What this means:\n",
    "- We only look at the **first 10 recommendations**\n",
    "- We calculate precision at each position (1st, 2nd, 3rd, ... 10th)\n",
    "- We only count positions where we made a **correct recommendation**\n",
    "- We normalize by the number of possible relevant items (max 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ’¡ Simple Example\n",
    "\n",
    "Let's say we have a user with **3 relevant items** in total, and our system makes these top-10 recommendations:\n",
    "\n",
    "| Position (k) | Recommended Item | Relevant? | rel_u(k) | Precision P_u(k) |\n",
    "|--------------|------------------|-----------|----------|------------------|\n",
    "| 1            | Item A           | âœ… Yes    | 1        | 1/1 = 1.00       |\n",
    "| 2            | Item B           | âŒ No     | 0        | 1/2 = 0.50       |\n",
    "| 3            | Item C           | âœ… Yes    | 1        | 2/3 = 0.67       |\n",
    "| 4            | Item D           | âŒ No     | 0        | 2/4 = 0.50       |\n",
    "| 5            | Item E           | âœ… Yes    | 1        | 3/5 = 0.60       |\n",
    "| 6-10         | Items F-J        | âŒ No     | 0        | ...              |\n",
    "\n",
    "### Calculation:\n",
    "```\n",
    "AP@10 = (1/min(3,10)) Ã— [1Ã—1.00 + 0Ã—0.50 + 1Ã—0.67 + 0Ã—0.50 + 1Ã—0.60 + 0Ã—...]\n",
    "AP@10 = (1/3) Ã— [1.00 + 0.67 + 0.60]\n",
    "AP@10 = (1/3) Ã— 2.27 = 0.757\n",
    "```\n",
    "\n",
    "**This user gets an AP@10 score of 0.757**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸª Visual Representation\n",
    "\n",
    "Let's create a simple visualization to understand how MAP@10 works:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T14:42:34.721209Z",
     "start_time": "2025-06-14T14:42:34.536922Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example recommendation scenario\n",
    "positions = np.arange(1, 11)\n",
    "recommendations = ['Item A', 'Item B', 'Item C', 'Item D', 'Item E', \n",
    "                  'Item F', 'Item G', 'Item H', 'Item I', 'Item J']\n",
    "relevant = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0]  # 1 = relevant, 0 = not relevant\n",
    "colors = ['green' if r else 'red' for r in relevant]\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Top plot: Recommendations with relevance\n",
    "bars1 = ax1.bar(positions, [1]*10, color=colors, alpha=0.7)\n",
    "ax1.set_xlabel('Recommendation Position')\n",
    "ax1.set_ylabel('Recommendations')\n",
    "ax1.set_title('Top-10 Recommendations (Green = Relevant, Red = Not Relevant)')\n",
    "ax1.set_xticks(positions)\n",
    "ax1.set_ylim(0, 1.2)\n",
    "\n",
    "# Add item labels\n",
    "for i, (pos, item) in enumerate(zip(positions, recommendations)):\n",
    "    ax1.text(pos, 0.5, item, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Bottom plot: Cumulative precision\n",
    "cumulative_hits = np.cumsum(relevant)\n",
    "precision_at_k = cumulative_hits / positions\n",
    "weighted_precision = [p * r for p, r in zip(precision_at_k, relevant)]\n",
    "\n",
    "ax2.plot(positions, precision_at_k, 'bo-', linewidth=2, markersize=6, label='Precision@k')\n",
    "ax2.bar(positions, weighted_precision, alpha=0.5, color='orange', \n",
    "        label='Weighted Precision (only counted when relevant)')\n",
    "ax2.set_xlabel('Position k')\n",
    "ax2.set_ylabel('Precision Value')\n",
    "ax2.set_title('Precision@k and Weighted Precision for MAP@10 Calculation')\n",
    "ax2.set_xticks(positions)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display the final AP@10\n",
    "relevant_items_count = sum(relevant)\n",
    "ap_10 = sum(weighted_precision) / min(relevant_items_count, 10)\n",
    "print(f\"\\nğŸ“Š Calculation Results:\")\n",
    "print(f\"Number of relevant items found: {relevant_items_count}\")\n",
    "print(f\"Sum of weighted precisions: {sum(weighted_precision):.3f}\")\n",
    "print(f\"AP@10 for this user: {ap_10:.3f}\")\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Example recommendation scenario\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ”‘ Key Insights\n",
    "\n",
    "### Why MAP@10 is Important:\n",
    "\n",
    "1. **ğŸ¯ Position Matters**: Earlier relevant recommendations contribute more to the score\n",
    "2. **âš–ï¸ Balanced Metric**: Considers both precision and the ranking order\n",
    "3. **ğŸ“ Standardized**: Scores range from 0 to 1, making comparison easy\n",
    "4. **ğŸ‘¥ User-Centric**: Calculated per user, then averaged across all users\n",
    "\n",
    "### What Makes a Good MAP@10 Score:\n",
    "\n",
    "- **Perfect Score (1.0)**: All relevant items appear at the very beginning of recommendations\n",
    "- **Good Score (0.7-0.9)**: Most relevant items appear in early positions\n",
    "- **Poor Score (0.0-0.3)**: Few relevant items found, or they appear late in the ranking\n",
    "\n",
    "### ğŸ“ˆ To Improve MAP@10:\n",
    "- Focus on getting relevant items **higher in the ranking**\n",
    "- Improve the **quality** of top recommendations\n",
    "- Balance between **precision** and **recall** in your recommendation system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ† Summary\n",
    "\n",
    "**MAP@10** is the evaluation metric for this competition. It measures how well our recommendation system ranks relevant items within the top 10 positions.\n",
    "\n",
    "### The Final Formula:\n",
    "```\n",
    "MAP@10 = Average of AP@10 scores across all users\n",
    "```\n",
    "\n",
    "Where each user's AP@10 considers:\n",
    "- âœ… **Which** of our top-10 recommendations are correct\n",
    "- ğŸ“ **Where** these correct recommendations appear in the ranking\n",
    "- ğŸ¯ **How many** relevant items the user actually has\n",
    "\n",
    "### ğŸ’¡ Remember:\n",
    "- **Higher is better** (max score = 1.0)\n",
    "- **Position matters** - relevant items should appear early\n",
    "- **Every user counts** - we average across all users in the dataset\n",
    "\n",
    "This metric ensures we build recommendation systems that are both **accurate** and **well-ordered**! ğŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
